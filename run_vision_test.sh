#!/bin/bash

# å®šä¹‰å¸¸é‡å‚æ•°
BACKEND="openai-chat"
HOST="127.0.0.1"
#HOST="10.103.10.74"
PORT="5000"

#!/bin/bash

# æª¢æŸ¥ jq æ˜¯å¦å­˜åœ¨
if ! command -v jq &> /dev/null; then
    echo "â— jq æœªå®‰è£ï¼Œæ­£åœ¨é€²è¡Œå®‰è£..."

    # æª¢æŸ¥ OS é¡å‹ä¸¦é¸æ“‡é©ç•¶çš„å¥—ä»¶ç®¡ç†å™¨
    if [ -f /etc/debian_version ]; then
        echo "ğŸ” åµæ¸¬åˆ° Debian/Ubuntu ç³»çµ±ï¼Œä½¿ç”¨ apt å®‰è£ jq..."
        apt update && apt install -y jq
    elif [ -f /etc/redhat-release ]; then
        echo "ğŸ” åµæ¸¬åˆ° RHEL/CentOS ç³»çµ±ï¼Œä½¿ç”¨ yum å®‰è£ jq..."
        yum install -y epel-release && yum install -y jq
    elif command -v apk &> /dev/null; then
        echo "ğŸ” åµæ¸¬åˆ° Alpine Linuxï¼Œä½¿ç”¨ apk å®‰è£ jq..."
        apk add --no-cache jq
    else
        echo "ğŸš« ç„¡æ³•è‡ªå‹•åˆ¤æ–·å¥—ä»¶ç®¡ç†å™¨ï¼Œè«‹æ‰‹å‹•å®‰è£ jqã€‚"
        exit 1
    fi

    # å†æ¬¡æª¢æŸ¥ jq æ˜¯å¦æˆåŠŸå®‰è£
    if command -v jq &> /dev/null; then
        echo "âœ… jq å®‰è£å®Œæˆã€‚"
    else
        echo "âŒ jq å®‰è£å¤±æ•—ï¼Œè«‹æ‰‹å‹•è™•ç†ã€‚"
        exit 1
    fi
else
    echo "âœ… jq å·²å®‰è£ï¼š$(jq --version)"
fi


# å‹•æ…‹ç²å– served-model-name
response=$(curl -s http://${HOST}:${PORT}/v1/models)
MODEL_NAME=$(echo "$response" | jq -r '.data[0].id')
#MODEL_NAME=$(ps aux | grep -E "python3 -m (vllm|vllm_ocisext).entrypoints.openai.api_server" | grep -oP "(?<=--served-model-name )[^\s]+" | head -n 1)
#TOKENIZER="fake_model"
TOKENIZER=$(echo "$response" | jq -r '.data[0].root')
ENDPOINT="/v1/chat/completions"
DATASET_NAME="ocisvision"
REQUEST_RATE="inf"
IM_WIDTH="512"
IM_HEIGHT="512"
# ä½¿ç”¨ä¼ å…¥çš„å‚æ•°è®¾ç½® IM_WIDTH å’Œ IM_HEIGHTï¼Œé»˜è®¤å€¼ä¸º 512
IM_WIDTH="${1:-512}"
IM_HEIGHT="${2:-512}"

echo "Image Width: $IM_WIDTH"
echo "Image Height: $IM_HEIGHT"

#RANDOM_INPUT_LEN="129024"
# å‹•æ…‹ç²å– max-model-len ä¸¦è¨­ç½®ç‚º RANDOM_INPUT_LENï¼Œä¸¦æ¸›å» 2048
MAX_MODEL_LEN=$(ps aux | grep -E "python3 -m (vllm|vllm_ocisext).entrypoints.openai.api_server" | sed -n 's/.*--max-model-len[= ]\([0-9]\+\).*/\1/p' | head -n 1)

# æª¢æŸ¥æ˜¯å¦æˆåŠŸå–å¾— MAX_MODEL_LEN ä¸¦é€²è¡Œæ¸›æ³•æ“ä½œ
if [ -z "$MAX_MODEL_LEN" ]; then
  echo "ç„¡æ³•å–å¾— max-model-lenï¼Œè«‹ç¢ºä¿è©²é€²ç¨‹æ­£åœ¨é‹è¡Œã€‚"
#  exit 1
fi

#RANDOM_INPUT_LEN=$((MAX_MODEL_LEN - 2048))
#RANDOM_OUTPUT_LEN="2048"

MEM_USAGE_OUT="output/mem_usage.txt"
ENGINE_PARAMS_OUT="output/engine_params.txt"

# ä½¿ç”¨ nvidia-smi æå– GPU åç¨±
GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader,nounits)

# å°‡ GPU åç¨±å¯«å…¥åˆ°æ–‡ä»¶ä¸­
echo "GPU Name: $GPU_NAME" > $MEM_USAGE_OUT

# å°‡ warm up è¨»è§£å’Œ Memory-Usage è¿½åŠ åˆ° mem_usage æª”æ¡ˆä¸­
echo "Warm up status:" >> $MEM_USAGE_OUT
nvidia-smi | grep "MiB /" | awk '{print $9, $11}' >> $MEM_USAGE_OUT

# é¡¯ç¤ºå­˜æª”æˆåŠŸçš„è¨Šæ¯
echo "Memory usage information (warm up status) has been appended to mem_usage."

# å°‡ engine params è¨»è§£å’Œ vllmåƒæ•¸ è¿½åŠ åˆ° engine_params æª”æ¡ˆä¸­
echo "engine params:" > $ENGINE_PARAMS_OUT
ps aux | grep -E "python3 -m (vllm|vllm_ocisext).entrypoints.openai.api_server" | grep -oP "(--\S+ \S+|--\S+)" >> $ENGINE_PARAMS_OUT 

echo "åƒæ•¸å·²æˆåŠŸæå–ä¸¦ä¿å­˜åˆ° params.txt æ–‡ä»¶ä¸­ã€‚"

# å®šä¹‰ä¸åŒçš„ --num-prompts å‚æ•°
NUM_PROMPTS_LIST=(8 16 32 64 128)
OV_INPUT_LEN="50"
OV_OUTPUT_LEN="300"

# å¾ªç¯æ‰§è¡Œä¸‰æ¬¡ï¼Œæ¯æ¬¡ä½¿ç”¨ä¸åŒçš„ --num-prompts
for NUM_PROMPTS in "${NUM_PROMPTS_LIST[@]}"; do

    RESULT_FILE="${MODEL_NAME}_${OV_INPUT_LEN}-${OV_OUTPUT_LEN}_${NUM_PROMPTS}.json"
    echo "Running benchmark with --num-prompts=$NUM_PROMPTS"
    python3 benchmark_serving.py --backend "$BACKEND" \
                                 --host "$HOST" \
                                 --port "$PORT" \
                                 --model "$MODEL_NAME" \
				 --tokenizer "$TOKENIZER" \
                                 --endpoint "$ENDPOINT" \
                                 --dataset-name "$DATASET_NAME" \
                                 --request-rate "$REQUEST_RATE" \
                                 --num-prompts "$NUM_PROMPTS" \
                                 --ov-input-len "$OV_INPUT_LEN" \
                                 --ov-output-len "$OV_OUTPUT_LEN" \
				 --im-width "$IM_WIDTH" \
				 --im-height "$IM_HEIGHT" \
    				 --save-result \
				 --result-dir "output" \
				 --result-filename "$RESULT_FILE"
done

NUM_PROMPTS_LIST=(1 8 16 32)
OV_INPUT_LEN="100"
OV_OUTPUT_LEN="100"

# å¾ªç¯æ‰§è¡Œä¸‰æ¬¡ï¼Œæ¯æ¬¡ä½¿ç”¨ä¸åŒçš„ --num-prompts
for NUM_PROMPTS in "${NUM_PROMPTS_LIST[@]}"; do

    RESULT_FILE="${MODEL_NAME}_${OV_INPUT_LEN}-${OV_OUTPUT_LEN}_${NUM_PROMPTS}.json"
    echo "Running benchmark with --num-prompts=$NUM_PROMPTS"
    python3 benchmark_serving.py --backend "$BACKEND" \
                                 --host "$HOST" \
                                 --port "$PORT" \
                                 --model "$MODEL_NAME" \
				 --tokenizer "$TOKENIZER" \
                                 --endpoint "$ENDPOINT" \
                                 --dataset-name "$DATASET_NAME" \
                                 --request-rate "$REQUEST_RATE" \
                                 --num-prompts "$NUM_PROMPTS" \
                                 --ov-input-len "$OV_INPUT_LEN" \
                                 --ov-output-len "$OV_OUTPUT_LEN" \
				 --im-width "$IM_WIDTH" \
				 --im-height "$IM_HEIGHT" \
    				 --save-result \
				 --result-dir "output" \
				 --result-filename "$RESULT_FILE"
done


# å°‡ After Benchmark è¨»è§£å’Œ Memory-Usage è¿½åŠ åˆ° mem_usage æª”æ¡ˆä¸­
echo "After Benchmark status:" >> $MEM_USAGE_OUT
nvidia-smi | grep "MiB /" | awk '{print $9, $11}' >> $MEM_USAGE_OUT

# é¡¯ç¤ºå­˜æª”æˆåŠŸçš„è¨Šæ¯
echo "Memory usage information (After Benchmark status) has been appended to mem_usage."
